{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-02T04:26:01.766012Z","iopub.execute_input":"2024-04-02T04:26:01.766352Z","iopub.status.idle":"2024-04-02T04:26:01.772278Z","shell.execute_reply.started":"2024-04-02T04:26:01.766322Z","shell.execute_reply":"2024-04-02T04:26:01.771429Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\n# from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-02T04:26:01.773836Z","iopub.execute_input":"2024-04-02T04:26:01.774112Z","iopub.status.idle":"2024-04-02T04:26:10.273162Z","shell.execute_reply.started":"2024-04-02T04:26:01.774088Z","shell.execute_reply":"2024-04-02T04:26:10.272384Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nOver the course of this experiment, I found that these AI models, are great at making believable text, but aren't great at fullfilling the task of finishing the story.","metadata":{}},{"cell_type":"markdown","source":"# Choice of Text and Models\n\nI chose the ROCStories data set, which is a data set (seen below) that has a lot of short stories.\n\nI chose these two models because they seem like powerful text models that would successfully accomplish the task set out for them.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('https://goo.gl/0OYkPK')\ndf.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T04:26:10.274167Z","iopub.execute_input":"2024-04-02T04:26:10.274639Z","iopub.status.idle":"2024-04-02T04:26:16.066858Z","shell.execute_reply.started":"2024-04-02T04:26:10.274613Z","shell.execute_reply":"2024-04-02T04:26:16.065985Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                storyid              storytitle  \\\n0  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd  David Drops the Weight   \n\n                                           sentence1  \\\n0  David noticed he had put on a lot of weight re...   \n\n                                           sentence2  \\\n0  He examined his habits to try and figure out t...   \n\n                                           sentence3  \\\n0  He realized he'd been eating too much fast foo...   \n\n                                           sentence4  \\\n0  He stopped going to burger places and started ...   \n\n                                           sentence5  \n0  After a few weeks, he started to feel much bet...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>storyid</th>\n      <th>storytitle</th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>sentence3</th>\n      <th>sentence4</th>\n      <th>sentence5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd</td>\n      <td>David Drops the Weight</td>\n      <td>David noticed he had put on a lot of weight re...</td>\n      <td>He examined his habits to try and figure out t...</td>\n      <td>He realized he'd been eating too much fast foo...</td>\n      <td>He stopped going to burger places and started ...</td>\n      <td>After a few weeks, he started to feel much bet...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Findings","metadata":{}},{"cell_type":"markdown","source":"| Text | Model | Accuracy | Loss | Perplexity |\n| - | - | - | - | - |\n| story1 | GPT-2 | Repeated the last sentence a lot| 2.986330270767212 | 19.812841415405273 |\n| story1 | Phi-2 | The text is exactly the same. | 2.1900503635406494 | 8.935663223266602 |\n| story2 | GPT-2 | Started to give different variations of the a sentence till it started to repeat | 3.617241859436035 | 37.23472595214844 |\n| story2 | Phi-2 | Made a quiz question out of the text. | 2.820509910583496 | 16.7854061126709 |","metadata":{}},{"cell_type":"markdown","source":"## Interpretation\n\nThe perplexity shows that GPT-2 has a lot more reasonable options, each step of the way than compared to the Phi-2 model.\n\nWhen looking at the loss, we can see that the GPT-2 model also has a higher average loss over the course of the 2 stories.\n\nLooking at the accuracy, we can see that Phi-2 did better in this regard as well. GPT-2 kept repeating the last sentence while Phi-2 might not have finished properly but it rather made a quiz question out of it.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nOverall, I think that the Phi-2 model is better than the GPT-2 model for this test. It shows across all of the different categories that it is better in this usecase.","metadata":{}},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\n# Define the text you want to evaluate\n# text = \"Once upon a time, there was a bug.\"\ntext = df[\"storytitle\"][1]\n\n# Define additional context\nadditional_text = df['sentence1'][1] + df['sentence2'][1] + df['sentence3'][1] + df['sentence4'][1] + df['sentence5'][1]\n\n# Combine the input text and additional context\ntext_with_context = f\"{text} {additional_text}\"\n\n# Define the model and tokenizer\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_id = \"openai-community/gpt2\"\nmodel = GPT2LMHeadModel.from_pretrained(model_id).to(device)\ntokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n\ntry:\n    # Tokenize the text with context\n    inputs = tokenizer(text_with_context, return_tensors=\"pt\", max_length=100, truncation=True)\n\n    # Prepare the input tensor\n    input_ids = inputs.input_ids.to(device)\n\n    # Prepare the target labels, ignoring the prompt\n    prompt_len = len(tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0])\n    target_ids = input_ids.clone()\n    target_ids[:, :prompt_len] = -100\n\n    # Compute the loss\n    with torch.no_grad():\n        outputs = model(input_ids, labels=target_ids)\n        neg_log_likelihood = outputs.loss\n\n    # Compute perplexity\n    perplexity = torch.exp(neg_log_likelihood)\n\n    print(f\"Loss: {neg_log_likelihood.item()}\")\n    print(f\"Perplexity: {perplexity.item()}\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Debugging outputs\nprint(\"Input Text with Context:\", text_with_context)\nprint(\"Tokenized Input IDs:\", input_ids)\nprint(\"Target IDs:\", target_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T04:26:16.078007Z","iopub.execute_input":"2024-04-02T04:26:16.078263Z","iopub.status.idle":"2024-04-02T04:26:30.267795Z","shell.execute_reply.started":"2024-04-02T04:26:16.078242Z","shell.execute_reply":"2024-04-02T04:26:30.266825Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8847e282815486d8ff95a17e1dacbd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6769961d919450cbc4c2229fa9b6678"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b2f8959524743ea9c1fa5b16348f81f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d1f481080f241e9889585d71af42bc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f22de9776f354f02a87b15e602125484"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb9c84767ef45049a96be54d0cd671a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34013ed5ceea4ae4b110870649f03e4d"}},"metadata":{}},{"name":"stdout","text":"Loss: 3.617241859436035\nPerplexity: 37.23472595214844\nInput Text with Context: Frustration Tom had a very short temper.One day a guest made him very angry.He punched a hole in the wall of his house.Tom's guest became afraid and left quickly.Tom sat on his couch filled with regret about his actions.\nTokenized Input IDs: tensor([[ 6732, 44027,  4186,   550,   257,   845,  1790,  4124,    13,  3198,\n          1110,   257,  8319,   925,   683,   845,  7954,    13,  1544, 25436,\n           257,  7604,   287,   262,  3355,   286,   465,  2156,    13, 13787,\n           338,  8319,  2627,  7787,   290,  1364,  2952,    13, 13787,  3332,\n           319,   465, 18507,  5901,   351, 13721,   546,   465,  4028,    13]],\n       device='cuda:0')\nTarget IDs: tensor([[ -100,  -100,  4186,   550,   257,   845,  1790,  4124,    13,  3198,\n          1110,   257,  8319,   925,   683,   845,  7954,    13,  1544, 25436,\n           257,  7604,   287,   262,  3355,   286,   465,  2156,    13, 13787,\n           338,  8319,  2627,  7787,   290,  1364,  2952,    13, 13787,  3332,\n           319,   465, 18507,  5901,   351, 13721,   546,   465,  4028,    13]],\n       device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Generate text\nwith torch.no_grad():\n    output = model.generate(input_ids=input_ids, max_length=150, num_return_sequences=1)\n\n# Decode the generated output\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(\"Generated Output Text:\", output_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T04:26:30.268993Z","iopub.execute_input":"2024-04-02T04:26:30.269347Z","iopub.status.idle":"2024-04-02T04:26:32.719733Z","shell.execute_reply.started":"2024-04-02T04:26:30.269316Z","shell.execute_reply":"2024-04-02T04:26:32.718841Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Output Text: Frustration Tom had a very short temper.One day a guest made him very angry.He punched a hole in the wall of his house.Tom's guest became afraid and left quickly.Tom sat on his couch filled with regret about his actions.Tom was a very good student.He was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a good student.Tom was a\n","output_type":"stream"}]}]}