{"metadata":{"colab":{"authorship_tag":"ABX9TyNS7mRS03a7VSFcbdUnYf/k","collapsed_sections":[],"include_colab_link":true,"name":"012-tokenization.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenization\n\nTask: Convert text to numbers; interpret subword tokenization.\n\nThere are various different ways of converting text to numbers. This assignment works with one popular approach: assign numbers to parts of words.","metadata":{"id":"3jc8Qlh1TEgC"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"f8_8RWp3TX-8"}},{"cell_type":"markdown","source":"We'll be using the HuggingFace Transformers library, which provides a (mostly) consistent interface to many different language models. We'll focus on the OpenAI GPT-2 model, famous for OpenAI's assertion that it was \"too dangerous\" to release in full.\n\n- [Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for the model and tokenizer.\n- [Model Card](https://github.com/openai/gpt-2/blob/master/model_card.md) for GPT-2.","metadata":{"id":"aUvTIxyWTdBF"}},{"cell_type":"markdown","source":"The `transformers` library is pre-installed on many systems, but in case you need to install it, you can run the following cell.","metadata":{}},{"cell_type":"code","source":"# Uncomment the following line to install the transformers library\n#!pip install -q transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWy--2nwhWPy","outputId":"44b8e674-7e8b-4cf6-a1e9-1f8d62740382","execution":{"iopub.status.busy":"2024-03-15T15:07:22.756120Z","iopub.execute_input":"2024-03-15T15:07:22.756684Z","iopub.status.idle":"2024-03-15T15:07:22.793830Z","shell.execute_reply.started":"2024-03-15T15:07:22.756629Z","shell.execute_reply":"2024-03-15T15:07:22.792501Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import tensor","metadata":{"id":"osKgPaDwhaN4","execution":{"iopub.status.busy":"2024-03-15T15:07:22.795876Z","iopub.execute_input":"2024-03-15T15:07:22.796250Z","iopub.status.idle":"2024-03-15T15:07:26.977827Z","shell.execute_reply.started":"2024-03-15T15:07:22.796219Z","shell.execute_reply":"2024-03-15T15:07:26.976622Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Download and load the model\n\nThis cell downloads the model and tokenizer, and loads them into memory.","metadata":{"id":"UiNKbIh8hyDg"}},{"cell_type":"code","source":"# https://huggingface.co/docs/transformers/en/generation_strategies\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, set_seed\nmodel_name = \"openai-community/gpt2\"\n# Here's a few larger models you could try:\n# model_name = \"EleutherAI/pythia-1.4b-deduped\"\n# model_name = \"google/gemma-2b\"\n# model_name = \"google/gemma-2b-it\"\n# Note: you'll need to accept the license agreement on https://huggingface.co/google/gemma-7b to use Gemma models\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n\n# add the EOS token as PAD token to avoid warnings\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nif model.generation_config.pad_token_id is None:\n    model.generation_config.pad_token_id = model.generation_config.eos_token_id\nstreamer = TextStreamer(tokenizer)\n# Silence a warning.\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"id":"IM5o_4w1hfyV","execution":{"iopub.status.busy":"2024-03-15T15:07:26.978984Z","iopub.execute_input":"2024-03-15T15:07:26.979381Z","iopub.status.idle":"2024-03-15T15:07:52.392916Z","shell.execute_reply.started":"2024-03-15T15:07:26.979355Z","shell.execute_reply":"2024-03-15T15:07:52.391878Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"520255600d2443fbac7140203dd19e79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b45ce3d6d1e44ec5ae63ee3e925802b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6692ce5d3104bdd9316d14195e9ea4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"428bbb42c0144a3892e90c5dc31a20bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c17989ed67b442db7d0e2b5c0cbc345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7233ce289fc408f9917a5fd21835fe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da067cc8752c440c9afce655cac858f0"}},"metadata":{}},{"name":"stderr","text":"2024-03-15 15:07:41.081332: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-15 15:07:41.081504: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-15 15:07:41.267670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"token_to_id_dict = tokenizer.get_vocab()\nprint(f\"The tokenizer has {len(token_to_id_dict)} strings in its vocabulary.\")\nprint(f\"The model has {model.num_parameters():,d} parameters.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-Z9_U0LUEVQ","outputId":"1d639faf-5b56-4bb2-81e5-054ee086ef0a","execution":{"iopub.status.busy":"2024-03-15T15:07:52.394611Z","iopub.execute_input":"2024-03-15T15:07:52.395338Z","iopub.status.idle":"2024-03-15T15:07:52.443719Z","shell.execute_reply.started":"2024-03-15T15:07:52.395305Z","shell.execute_reply":"2024-03-15T15:07:52.442160Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The tokenizer has 50257 strings in its vocabulary.\nThe model has 124,439,808 parameters.\n","output_type":"stream"}]},{"cell_type":"code","source":"# warning: this assumes that there are no gaps in the token ids, which happens to be true for this tokenizer.\nid_to_token = [token for token, id in sorted(token_to_id_dict.items(), key=lambda x: x[1])]\nprint(f\"The first 10 tokens are: {id_to_token[:10]}\")\nprint(f\"The last 10 tokens are: {id_to_token[-10:]}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:07:52.450811Z","iopub.execute_input":"2024-03-15T15:07:52.451302Z","iopub.status.idle":"2024-03-15T15:07:53.811734Z","shell.execute_reply.started":"2024-03-15T15:07:52.451253Z","shell.execute_reply":"2024-03-15T15:07:53.810682Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The first 10 tokens are: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\nThe last 10 tokens are: ['Ġ(/', 'âĢ¦.\"', 'Compar', 'Ġamplification', 'ominated', 'Ġregress', 'ĠCollider', 'Ġinformants', 'Ġgazed', '<|endoftext|>']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Demo","metadata":{}},{"cell_type":"code","source":"set_seed(0)\nmodel.generate(\n    **tokenizer(\"A list of colors: red, blue,\", return_tensors=\"pt\"),\n    max_new_tokens=10, do_sample=True, temperature=0.3, penalty_alpha=.5, top_k=5, streamer=streamer);","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:07:53.813108Z","iopub.execute_input":"2024-03-15T15:07:53.813806Z","iopub.status.idle":"2024-03-15T15:07:54.441911Z","shell.execute_reply.started":"2024-03-15T15:07:53.813776Z","shell.execute_reply":"2024-03-15T15:07:54.440891Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":" A list of colors: red, blue, green, yellow, orange, yellow, orange,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Task\n\nConsider the following phrase:","metadata":{"id":"OOUiz_PsUZgS"}},{"cell_type":"code","source":"phrase = \"I visited Muskegon\"\n# Another one to try later. This was a famous early example of the GPT-2 model:\n# phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\"","metadata":{"id":"JS7Z-DjoUiLK","execution":{"iopub.status.busy":"2024-03-15T15:07:54.445410Z","iopub.execute_input":"2024-03-15T15:07:54.446035Z","iopub.status.idle":"2024-03-15T15:07:54.450400Z","shell.execute_reply.started":"2024-03-15T15:07:54.446002Z","shell.execute_reply":"2024-03-15T15:07:54.449499Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Getting familiar with tokens\n\n1: Use `tokenizer.tokenize` to convert the phrase into a list of tokens. (What do you think the `Ġ` means?)","metadata":{}},{"cell_type":"code","source":"tokens = tokenizer.tokenize(phrase)\ntokens","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hyq-5XWSUx_8","outputId":"22efb7a8-37c5-46f0-e230-c3b8e5ad6bdc","execution":{"iopub.status.busy":"2024-03-15T15:07:54.451792Z","iopub.execute_input":"2024-03-15T15:07:54.452105Z","iopub.status.idle":"2024-03-15T15:07:54.467201Z","shell.execute_reply.started":"2024-03-15T15:07:54.452078Z","shell.execute_reply":"2024-03-15T15:07:54.466045Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['ĠI', 'Ġvisited', 'ĠMus', 'ke', 'gon']"},"metadata":{}}]},{"cell_type":"markdown","source":"2: Use `tokenizer.convert_tokens_to_string` to convert the tokens back into a string.\n","metadata":{}},{"cell_type":"code","source":"# your code here\ntokenizer.convert_tokens_to_string(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:08:56.495608Z","iopub.execute_input":"2024-03-15T15:08:56.496027Z","iopub.status.idle":"2024-03-15T15:08:56.503334Z","shell.execute_reply.started":"2024-03-15T15:08:56.495998Z","shell.execute_reply":"2024-03-15T15:08:56.502503Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"code","source":"# for comparison:\n''.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:07:54.478193Z","iopub.execute_input":"2024-03-15T15:07:54.478548Z","iopub.status.idle":"2024-03-15T15:07:54.489243Z","shell.execute_reply.started":"2024-03-15T15:07:54.478508Z","shell.execute_reply":"2024-03-15T15:07:54.487996Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'ĠIĠvisitedĠMuskegon'"},"metadata":{}}]},{"cell_type":"markdown","source":"What is the difference between the output from `convert_tokens_to_string` and the result of ''.join(tokens)?\n\nthe convert makes the tokens back into the original string, while join has some weird behaviour with spaces","metadata":{}},{"cell_type":"markdown","source":"3: Use `tokenizer.encode` to convert the original phrase into token ids. (*Note: this is equivalent to `tokenize` followed by `convert_tokens_to_ids`*.) Call the result `input_ids`.\n","metadata":{}},{"cell_type":"code","source":"input_ids = tokenizer.encode(phrase)\ninput_ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkaoLSFMiHzb","outputId":"18c6391e-a9aa-4d4c-dace-d49f8bbcba7a","execution":{"iopub.status.busy":"2024-03-15T15:10:54.659784Z","iopub.execute_input":"2024-03-15T15:10:54.660681Z","iopub.status.idle":"2024-03-15T15:10:54.667898Z","shell.execute_reply.started":"2024-03-15T15:10:54.660644Z","shell.execute_reply":"2024-03-15T15:10:54.666887Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[314, 8672, 2629, 365, 14520]"},"metadata":{}}]},{"cell_type":"markdown","source":"4: Turn `input_ids` back into a readable string. Try this two ways: (1) using `tokenizer.decode` and (2) using `convert_ids_to_tokens`. **The result of (1) should be the same as the result of (2).**","metadata":{}},{"cell_type":"code","source":"# using convert_ids_to_tokens\n# your code here\ntokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:13:06.637505Z","iopub.execute_input":"2024-03-15T15:13:06.637935Z","iopub.status.idle":"2024-03-15T15:13:06.646193Z","shell.execute_reply.started":"2024-03-15T15:13:06.637901Z","shell.execute_reply":"2024-03-15T15:13:06.644947Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"code","source":"# using tokenizer.decode\n# your code here\ntokenizer.decode(input_ids)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ncSRaBaZix8R","outputId":"204670f9-d7c4-4856-c804-a038b77ccd1c","execution":{"iopub.status.busy":"2024-03-15T15:13:18.743636Z","iopub.execute_input":"2024-03-15T15:13:18.744074Z","iopub.status.idle":"2024-03-15T15:13:18.751988Z","shell.execute_reply.started":"2024-03-15T15:13:18.744038Z","shell.execute_reply":"2024-03-15T15:13:18.750666Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Applying what you learned\n\n5: Use `model.generate(input_ids_batch)` to generate a completion of this phrase. (Note that we needed to add `[]`s to give a \"batch\" dimension to the input, and convert the result to a PyTorch `tensor` for the model code to use it.) Call the result `output_ids`. This one is done for you.\n","metadata":{}},{"cell_type":"code","source":"input_ids_batch = tensor([input_ids])\noutput_ids = model.generate(input_ids_batch, max_new_tokens=20)\noutput_ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PZm3eIjjKCJ","outputId":"1c4b1a63-de00-44f9-eee7-85714012dbc6","execution":{"iopub.status.busy":"2024-03-15T15:13:25.221101Z","iopub.execute_input":"2024-03-15T15:13:25.221788Z","iopub.status.idle":"2024-03-15T15:13:26.092111Z","shell.execute_reply.started":"2024-03-15T15:13:25.221740Z","shell.execute_reply":"2024-03-15T15:13:26.090944Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"tensor([[  314,  8672,  2629,   365, 14520,    11,   290,   314,   373,  1297,\n           326,   262,  1748,   373,   287,   262,  1429,   286,   852,  3170,\n            13,   314,   373,  1297,   326]])"},"metadata":{}}]},{"cell_type":"markdown","source":"6: Convert your `output_ids` into a readable form. (Note: it has an extra \"batch\" dimension, so you'll need to use `output_ids[0]`.)","metadata":{}},{"cell_type":"code","source":"# your code here\ntokenizer.decode(output_ids[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"2kKJ8rvijVez","outputId":"386df167-0e88-45f1-b1a0-01beab1f0bc8","execution":{"iopub.status.busy":"2024-03-15T15:13:49.057343Z","iopub.execute_input":"2024-03-15T15:13:49.057807Z","iopub.status.idle":"2024-03-15T15:13:49.065958Z","shell.execute_reply.started":"2024-03-15T15:13:49.057768Z","shell.execute_reply":"2024-03-15T15:13:49.064811Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon, and I was told that the city was in the process of being built. I was told that'"},"metadata":{}}]},{"cell_type":"markdown","source":"Note: `generate` uses a greedy decoding by default, but it's highly customizable. We'll play more with it in later exercises. For now, if you want more interesting results, try:\n\n- Turn on `do_sample=True`. Run it a few times to see what it gives. Try `temperature = 0.7` or `temperature = 1.5`.\n- With sampling enabled, set `top_k=5`. Or 50.","metadata":{}},{"cell_type":"markdown","source":"7. What is the largest possible token id for this tokenizer? What token does it correspond to? (Hint: at the top of the notebook, we printed out the size of the vocabulary.)\n\n50257 has that many tokens in its vocab, it corresponds to <|endoftext|>","metadata":{}},{"cell_type":"code","source":"# your code here\nprint(len(id_to_token))\nid_to_token[-1]","metadata":{"execution":{"iopub.status.busy":"2024-03-15T15:18:52.831772Z","iopub.execute_input":"2024-03-15T15:18:52.832187Z","iopub.status.idle":"2024-03-15T15:18:52.841079Z","shell.execute_reply.started":"2024-03-15T15:18:52.832156Z","shell.execute_reply":"2024-03-15T15:18:52.839862Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"50257\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Analysis","metadata":{}},{"cell_type":"markdown","source":"Q1: **Write a brief explanation of what a tokenizer does.** Note that we worked with two parts of a tokenizer in this exercise (one that deals only with strings, and another that deals with numbers); make sure your explanation addresses both parts.","metadata":{}},{"cell_type":"markdown","source":"It converts different elements of a string into a number respresentation of that section of the string","metadata":{}},{"cell_type":"markdown","source":"Q2: Suppose a language model has learned to spell, e.g., after the prefix \"The word dog is spelled d o g\". Will it then already know how to spell any other word, or does it have to re-learn spelling for each word? Why or why not? (For example, try tokenizing the phrase \"The word walking is spelled: w a\" and then asking the LM to complete it.)","metadata":{}},{"cell_type":"markdown","source":"It would need to relearn it for every word because the model can't see inside the token, it just sees a number","metadata":{}},{"cell_type":"markdown","source":"Q3: Suppose you made a typo in your input. Explain what the tokenizer we used in this notebook will do with the new input. (Go back and try an input with a typo to see what happens.)","metadata":{}},{"cell_type":"markdown","source":"It would make a new token for your typo and will see it as a new word","metadata":{}}]}